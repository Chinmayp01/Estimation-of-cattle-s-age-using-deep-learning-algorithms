{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JQ_tuPJyW9y-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import shutil\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please select the folder from the pop up:\n",
            "Selected folder: D:/Others/Project_Submission_ALL_DATA\n"
          ]
        }
      ],
      "source": [
        "from tkinter import Tk\n",
        "from tkinter.filedialog import askdirectory\n",
        "\n",
        "# Hide the main Tkinter window\n",
        "Tk().withdraw()\n",
        "# Print the selected folder\n",
        "print(f\"Please select the folder from the pop up:\")\n",
        "# Ask the user to select the folder with custom text\n",
        "home_folder = askdirectory(title=\"Select the Project_submission folder\")\n",
        "\n",
        "# Check if a folder was selected\n",
        "if not home_folder:\n",
        "    print(\"No folder selected. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "# Print the selected folder or use it in your script\n",
        "print(f\"Selected folder: {home_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z0Mk30MzXYTy",
        "outputId": "a5ae43f2-01b2-4e3f-a9df-cba23e8321ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 3 directories and 0 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS'.\n",
            "There are 15 directories and 0 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test'.\n",
            "There are 0 directories and 28 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 10 year'.\n",
            "There are 0 directories and 27 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 11 year'.\n",
            "There are 0 directories and 27 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 12 year'.\n",
            "There are 0 directories and 25 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 13 year'.\n",
            "There are 0 directories and 25 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 14 year'.\n",
            "There are 0 directories and 19 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 15 year'.\n",
            "There are 0 directories and 12 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 16 year'.\n",
            "There are 0 directories and 33 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 2 year'.\n",
            "There are 0 directories and 35 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 3 year'.\n",
            "There are 0 directories and 36 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 4 year'.\n",
            "There are 0 directories and 44 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 5 year'.\n",
            "There are 0 directories and 35 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 6 year'.\n",
            "There are 0 directories and 38 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 7 year'.\n",
            "There are 0 directories and 33 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 8 year'.\n",
            "There are 0 directories and 32 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\test\\up to 9 year'.\n",
            "There are 15 directories and 0 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train'.\n",
            "There are 0 directories and 128 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 10 year'.\n",
            "There are 0 directories and 118 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 11 year'.\n",
            "There are 0 directories and 120 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 12 year'.\n",
            "There are 0 directories and 109 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 13 year'.\n",
            "There are 0 directories and 109 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 14 year'.\n",
            "There are 0 directories and 81 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 15 year'.\n",
            "There are 0 directories and 51 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 16 year'.\n",
            "There are 0 directories and 149 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 2 year'.\n",
            "There are 0 directories and 160 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 3 year'.\n",
            "There are 0 directories and 163 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 4 year'.\n",
            "There are 0 directories and 198 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 5 year'.\n",
            "There are 0 directories and 158 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 6 year'.\n",
            "There are 0 directories and 173 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 7 year'.\n",
            "There are 0 directories and 149 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 8 year'.\n",
            "There are 0 directories and 142 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\train\\up to 9 year'.\n",
            "There are 15 directories and 0 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val'.\n",
            "There are 0 directories and 27 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 10 year'.\n",
            "There are 0 directories and 25 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 11 year'.\n",
            "There are 0 directories and 25 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 12 year'.\n",
            "There are 0 directories and 23 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 13 year'.\n",
            "There are 0 directories and 23 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 14 year'.\n",
            "There are 0 directories and 17 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 15 year'.\n",
            "There are 0 directories and 11 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 16 year'.\n",
            "There are 0 directories and 31 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 2 year'.\n",
            "There are 0 directories and 34 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 3 year'.\n",
            "There are 0 directories and 35 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 4 year'.\n",
            "There are 0 directories and 42 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 5 year'.\n",
            "There are 0 directories and 34 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 6 year'.\n",
            "There are 0 directories and 37 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 7 year'.\n",
            "There are 0 directories and 32 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 8 year'.\n",
            "There are 0 directories and 30 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS\\val\\up to 9 year'.\n",
            "Total Images : 2883\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "first_folder = os.path.join(home_folder,'DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS')\n",
        "\n",
        "def walk_through_dir(dir_path):\n",
        "  count_images = 0\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    count_images += len(filenames)\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "  print(f\"Total Images : {count_images}\")\n",
        "walk_through_dir(first_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gshQzH0QhMhY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Chinmay\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.models import VisionTransformer\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RNJFH5NCz5fZ"
      },
      "outputs": [],
      "source": [
        "def generate_loader( train_dir , test_dir , val_dir):\n",
        "\n",
        "    batch_size = 32\n",
        "    train_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataset = datasets.ImageFolder(val_dir, transform=val_transform)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    num_classes = len(train_dataset.classes)\n",
        "\n",
        "    return train_loader , test_loader , val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4Oji_2R0z6Vk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_pretrained_vision_transformer(model_name, num_classes, pretrained=True):\n",
        "    # Load pre-trained Vision Transformer model\n",
        "    model = timm.create_model(model_name, pretrained=pretrained)\n",
        "\n",
        "    # Modify classifier head for the specified number of classes\n",
        "    if hasattr(model, 'head'):\n",
        "        num_features = model.head.in_features\n",
        "        model.head = torch.nn.Linear(num_features, num_classes)\n",
        "    elif hasattr(model, 'classifier'):\n",
        "        num_features = model.classifier.in_features\n",
        "        model.classifier = torch.nn.Linear(num_features, num_classes)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model architecture, unable to modify classifier head.\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MSk6aeamz8lz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def test_model(model, test_loader, batch_size=32):\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    test_accuracy = correct / total\n",
        "    print(f'Testing Accuracy: {test_accuracy:.4f}')\n",
        "    return test_accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_accuracy_curves( train_accuracies, val_accuracies, save_path):\n",
        "    plt.figure(figsize=(4 , 4))\n",
        "    plt.rcParams['pdf.fonttype']=42\n",
        "    plt.rcParams['savefig.format'] = 'eps'\n",
        "    plt.rcParams['savefig.dpi'] = 420\n",
        "    plt.rcParams['font.family'] = 'sans-serif'\n",
        "    plt.rcParams['font.sans-serif'] = 'Arial'\n",
        "    plt.plot(train_accuracies, label='Training Accuracy', color='blue')\n",
        "    plt.plot(val_accuracies, label='Validation Accuracy', color='orange')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Vision Transformer')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    # Save the image of accuracy curves\n",
        "    #plt.savefig(save_path , format= 'png' , dpi = 420)\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iASsIr-Oz_s4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_metrics(model, test_loader):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgEh25D5_MOP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vNFSCOg_R0Y"
      },
      "source": [
        "***Vision Transformer Classfication Report without segmentation for 15 Classes***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1767569f6b38412f9f4c614898892c2c",
            "3b3d15197b8f44f7a3a258b6bf6a0350",
            "e56bda50d4cf4866a0f113e11ca21f85",
            "2118363ae8c04f4aaf0fc312c9f24bbb",
            "70057c1b6f424a1a922a9e899057a0f9",
            "c1c6e2b9331b46c4809871182ba6bd16",
            "ba8f390067134fbb9b32046a4d188f02",
            "7548d71ab1f04cbfbd25c4e310ea7be4",
            "72a0855e2e4543bfa5d4eb79a558ee5d",
            "d8158d493f0c4222bfaae2c35cde4f91",
            "fade0596b40941209f59f26fe4a374d4"
          ]
        },
        "id": "GmXSLkAt0BzZ",
        "outputId": "49bba827-1429-4174-a103-60d5d503505d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "  (patch_drop): Identity()\n",
              "  (norm_pre): Identity()\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (1): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (2): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (3): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (4): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (5): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (6): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (7): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (8): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (9): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (10): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (11): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  (fc_norm): Identity()\n",
              "  (head_drop): Dropout(p=0.0, inplace=False)\n",
              "  (head): Linear(in_features=768, out_features=15, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example usage:\n",
        "model_name = 'vit_base_patch16_224.augreg_in21k'\n",
        "num_classes = 15  # Assuming the number of classes is known\n",
        "pretrained = True   # Set to False if you don't want to use pre-trained weights\n",
        "model = load_pretrained_vision_transformer(model_name, num_classes, pretrained)\n",
        "\n",
        "# Load the saved state dictionary\n",
        "state_dict = torch.load(os.path.join(home_folder,'RESULTS/MODEL_FILES/Model_Files_15_Classes/vision_transformer_model_15_classes_ws.pth') , map_location='cpu')\n",
        "\n",
        "# Load the state dictionary into the model\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Now your model is loaded and ready to be used for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "e0wg09Wq0E6F"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(home_folder,\"DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS/train\")\n",
        "val_dir = os.path.join(home_folder,\"DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS/val\")\n",
        "test_dir = os.path.join(home_folder,\"DATASET_WS_VERIFICATION/Cattle Dataset 15 Classes WS/test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TV1LkOun0PdO"
      },
      "outputs": [],
      "source": [
        "train_loader , test_loader , val_loader = generate_loader( train_dir , test_dir , val_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EVjife10Rjc",
        "outputId": "a2745597-4758-4ace-c2ac-64e3a0472d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 0.3408\n",
            "Accuracy : 0.34075723830734966\n"
          ]
        }
      ],
      "source": [
        "test_accuracy = test_model(model, test_loader, batch_size=32)\n",
        "print(f\"Accuracy : {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bXKO1xsT0ZAr",
        "outputId": "96c9af39-32fa-470f-a762-3e85a00ab4a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.3356, Recall: 0.3408, F1-score: 0.3348\n"
          ]
        }
      ],
      "source": [
        "calculate_metrics(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL29eMLc-4vJ"
      },
      "source": [
        "***Vision Transformer Classfication Report without segmentation for 4 Classes***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbK70r8o0d7_",
        "outputId": "be1793c0-9425-4ba7-af5b-efc4e8783c09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 3 directories and 0 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS'.\n",
            "There are 4 directories and 0 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\test'.\n",
            "There are 0 directories and 144 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\test\\Group 1 (1 to 5 years)'.\n",
            "There are 0 directories and 135 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\test\\Group 2 (5.1 to 9 years)'.\n",
            "There are 0 directories and 103 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\test\\Group 3 (9.1 to 13 years)'.\n",
            "There are 0 directories and 53 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\test\\Group 4 (13.1 and above years)'.\n",
            "There are 4 directories and 0 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\train'.\n",
            "There are 0 directories and 672 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\train\\Group 1 (1 to 5 years)'.\n",
            "There are 0 directories and 625 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\train\\Group 2 (5.1 to 9 years)'.\n",
            "There are 0 directories and 477 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\train\\Group 3 (9.1 to 13 years)'.\n",
            "There are 0 directories and 243 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\train\\Group 4 (13.1 and above years)'.\n",
            "There are 4 directories and 0 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\val'.\n",
            "There are 0 directories and 144 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\val\\Group 1 (1 to 5 years)'.\n",
            "There are 0 directories and 133 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\val\\Group 2 (5.1 to 9 years)'.\n",
            "There are 0 directories and 102 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\val\\Group 3 (9.1 to 13 years)'.\n",
            "There are 0 directories and 52 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\\val\\Group 4 (13.1 and above years)'.\n",
            "Total Images : 2883\n"
          ]
        }
      ],
      "source": [
        "first_folder = os.path.join(home_folder,\"DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS\")\n",
        "walk_through_dir(first_folder)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIlVypl20d2k",
        "outputId": "4e958b47-9335-4b96-f6cf-a2b4cc5857a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "  (patch_drop): Identity()\n",
              "  (norm_pre): Identity()\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (1): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (2): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (3): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (4): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (5): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (6): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (7): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (8): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (9): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (10): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (11): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  (fc_norm): Identity()\n",
              "  (head_drop): Dropout(p=0.0, inplace=False)\n",
              "  (head): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example usage:\n",
        "model_name = 'vit_base_patch16_224.augreg_in21k'\n",
        "num_classes = 4  # Assuming the number of classes is known\n",
        "pretrained = True   # Set to False if you don't want to use pre-trained weights\n",
        "model = load_pretrained_vision_transformer(model_name, num_classes, pretrained)\n",
        "\n",
        "# Load the saved state dictionary\n",
        "state_dict = torch.load(os.path.join(home_folder,'RESULTS/MODEL_FILES/Model_Files_4_Classes/vision_transformer_model_4_classes_ws.pth') , map_location='cpu')\n",
        "\n",
        "# Load the state dictionary into the model\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Now your model is loaded and ready to be used for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MhU8xDFd0dzv"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(home_folder,\"DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS/train\")\n",
        "val_dir = os.path.join(home_folder,\"DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS/val\")\n",
        "test_dir = os.path.join(home_folder,\"DATASET_WS_VERIFICATION/Cattle Dataset 4 Classes WS/test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KwHYpNa60dwq"
      },
      "outputs": [],
      "source": [
        "train_loader , test_loader , val_loader = generate_loader( train_dir , test_dir , val_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eZxKFOw0dt-",
        "outputId": "9cb0b7e7-1626-44d9-eeab-1455cb2ede75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 0.6943\n",
            "Accuracy : 0.6942528735632184\n"
          ]
        }
      ],
      "source": [
        "test_accuracy = test_model(model, test_loader, batch_size=32)\n",
        "print(f\"Accuracy : {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXyy5D1P9Z3Q",
        "outputId": "a77d0840-86b7-4706-aa18-3de66c390cee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.6934, Recall: 0.6943, F1-score: 0.6934\n"
          ]
        }
      ],
      "source": [
        "calculate_metrics(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWuH4J3a-oIA"
      },
      "source": [
        "***Vision Transformer Classfication Report without segmentation for 7 Classes***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnhTDm-6-DRx",
        "outputId": "a567b953-dc0a-4ae4-b5f8-5ce98db36c58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 3 directories and 0 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS'.\n",
            "There are 7 directories and 0 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\test'.\n",
            "There are 0 directories and 67 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\test\\Group 1 (1 to 3 years)'.\n",
            "There are 0 directories and 79 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\test\\Group 2 (3.1 to 5 years)'.\n",
            "There are 0 directories and 72 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\test\\Group 3 (5.1 to 7 years)'.\n",
            "There are 0 directories and 64 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\test\\Group 4 (7.1 to 9 years)'.\n",
            "There are 0 directories and 54 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\test\\Group 5 (9.1 to 11 years)'.\n",
            "There are 0 directories and 50 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\test\\Group 6 (11.1 to 13 years)'.\n",
            "There are 0 directories and 53 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\test\\Group 7 (13.1 and above years)'.\n",
            "There are 7 directories and 0 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\train'.\n",
            "There are 0 directories and 309 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\train\\Group 1 (1 to 3 years)'.\n",
            "There are 0 directories and 362 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\train\\Group 2 (3.1 to 5 years)'.\n",
            "There are 0 directories and 332 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\train\\Group 3 (5.1 to 7 years)'.\n",
            "There are 0 directories and 292 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\train\\Group 4 (7.1 to 9 years)'.\n",
            "There are 0 directories and 247 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\train\\Group 5 (9.1 to 11 years)'.\n",
            "There are 0 directories and 230 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\train\\Group 6 (11.1 to 13 years)'.\n",
            "There are 0 directories and 243 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\train\\Group 7 (13.1 and above years)'.\n",
            "There are 7 directories and 0 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\val'.\n",
            "There are 0 directories and 66 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\val\\Group 1 (1 to 3 years)'.\n",
            "There are 0 directories and 77 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\val\\Group 2 (3.1 to 5 years)'.\n",
            "There are 0 directories and 71 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\val\\Group 3 (5.1 to 7 years)'.\n",
            "There are 0 directories and 62 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\val\\Group 4 (7.1 to 9 years)'.\n",
            "There are 0 directories and 52 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\val\\Group 5 (9.1 to 11 years)'.\n",
            "There are 0 directories and 49 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\val\\Group 6 (11.1 to 13 years)'.\n",
            "There are 0 directories and 52 images in 'D:/Others/Project_Submission_ALL_DATA\\DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\\val\\Group 7 (13.1 and above years)'.\n",
            "Total Images : 2883\n"
          ]
        }
      ],
      "source": [
        "first_folder = os.path.join(home_folder,\"DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS\")\n",
        "walk_through_dir(first_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fszq20RP-DOW",
        "outputId": "ef775d92-5a5e-497d-ee4a-3cf42ff90a9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "  (patch_drop): Identity()\n",
              "  (norm_pre): Identity()\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (1): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (2): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (3): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (4): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (5): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (6): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (7): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (8): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (9): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (10): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "    (11): Block(\n",
              "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): Attention(\n",
              "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "        (q_norm): Identity()\n",
              "        (k_norm): Identity()\n",
              "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): Identity()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (drop1): Dropout(p=0.0, inplace=False)\n",
              "        (norm): Identity()\n",
              "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (drop2): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): Identity()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  (fc_norm): Identity()\n",
              "  (head_drop): Dropout(p=0.0, inplace=False)\n",
              "  (head): Linear(in_features=768, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example usage:\n",
        "model_name = 'vit_base_patch16_224.augreg_in21k'\n",
        "num_classes = 7  # Assuming the number of classes is known\n",
        "pretrained = True   # Set to False if you don't want to use pre-trained weights\n",
        "model = load_pretrained_vision_transformer(model_name, num_classes, pretrained)\n",
        "\n",
        "# Load the saved state dictionary\n",
        "state_dict = torch.load(os.path.join(home_folder,'RESULTS/MODEL_FILES/Model_Files_7_Classes/vision_transformer_model_7_classes_ws.pth') , map_location='cpu')\n",
        "\n",
        "# Load the state dictionary into the model\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Now your model is loaded and ready to be used for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "y8d7TCRS-ec_"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(home_folder,\"DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS/train\")\n",
        "val_dir = os.path.join(home_folder,\"DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS/val\")\n",
        "test_dir = os.path.join(home_folder,\"DATASET_WS_VERIFICATION/Cattle Dataset 7 Classes WS/test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ekZnG6iq-ibK"
      },
      "outputs": [],
      "source": [
        "train_loader , test_loader , val_loader = generate_loader( train_dir , test_dir , val_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1G3jqRp-kTX",
        "outputId": "1ab0cc8f-a2a0-4166-f5f6-04e64da924ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 0.5604\n",
            "Accuracy : 0.5603644646924829\n"
          ]
        }
      ],
      "source": [
        "test_accuracy = test_model(model, test_loader, batch_size=32)\n",
        "print(f\"Accuracy : {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zi4V7fVA-mN2",
        "outputId": "a002c0db-ad37-432c-d5ce-86dadad3e53e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.5631, Recall: 0.5604, F1-score: 0.5609\n"
          ]
        }
      ],
      "source": [
        "calculate_metrics(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.24"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1767569f6b38412f9f4c614898892c2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b3d15197b8f44f7a3a258b6bf6a0350",
              "IPY_MODEL_e56bda50d4cf4866a0f113e11ca21f85",
              "IPY_MODEL_2118363ae8c04f4aaf0fc312c9f24bbb"
            ],
            "layout": "IPY_MODEL_70057c1b6f424a1a922a9e899057a0f9"
          }
        },
        "2118363ae8c04f4aaf0fc312c9f24bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8158d493f0c4222bfaae2c35cde4f91",
            "placeholder": "",
            "style": "IPY_MODEL_fade0596b40941209f59f26fe4a374d4",
            "value": "410M/410M[00:05&lt;00:00,22.2MB/s]"
          }
        },
        "3b3d15197b8f44f7a3a258b6bf6a0350": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1c6e2b9331b46c4809871182ba6bd16",
            "placeholder": "",
            "style": "IPY_MODEL_ba8f390067134fbb9b32046a4d188f02",
            "value": "model.safetensors:100%"
          }
        },
        "70057c1b6f424a1a922a9e899057a0f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72a0855e2e4543bfa5d4eb79a558ee5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7548d71ab1f04cbfbd25c4e310ea7be4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba8f390067134fbb9b32046a4d188f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1c6e2b9331b46c4809871182ba6bd16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8158d493f0c4222bfaae2c35cde4f91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e56bda50d4cf4866a0f113e11ca21f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7548d71ab1f04cbfbd25c4e310ea7be4",
            "max": 410397786,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72a0855e2e4543bfa5d4eb79a558ee5d",
            "value": 410397786
          }
        },
        "fade0596b40941209f59f26fe4a374d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
